# -*- coding: utf-8 -*-
"""[241207] 딥러닝 과제4 fine-tuning_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fqyOS1z8Bv0BPHfcalXJxLgXNWIN1xwC
"""

!pip install datasets

import torch
import transformers
import pandas as pd
from typing import Dict, List
from datasets import Dataset, load_dataset, disable_caching
disable_caching() ## disable huggingface cache
from transformers import AutoTokenizer, AutoModelForCausalLM
#from torch.utils.data import Dataset
#from IPython.display import Markdown

"""## 0. Data preprocessing
- 데이터 양으로 인해 train data과 validation data 병합
- 후에 train data, test data **split**
"""

import pandas as pd

# 두 데이터셋 로드 (training과 validation 데이터셋)
train_df = pd.read_csv("/content/final_Training_Counseling.csv", encoding='utf-8-sig')
valid_df = pd.read_csv("/content/final_Validation_Counseling.csv", encoding='utf-8-sig')

# crisis_level이 null인 데이터 제거
train_df = train_df.dropna(subset=['crisis_level','instruction','response'])
valid_df = valid_df.dropna(subset=['crisis_level','instruction','response'])

# training과 validation 데이터셋 병합
merged_df = pd.concat([train_df, valid_df], ignore_index=True)

# 병합된 데이터셋을 새로운 CSV 파일로 저장
output_file = "/content/drive/My Drive/Deep Learning/merged_Counseling_Dataset.csv"
merged_df.to_csv(output_file, index=False, encoding='utf-8')

# 병합된 데이터셋 확인
print(merged_df.head())

"""## 1. 웹으로 보일 수 있도록 구성하기"""

# Load the full dataset
df = pd.read_csv("/content/drive/My Drive/Deep Learning/241209_Counseling_Dataset_LLM.csv", encoding='utf-8')
dataset = Dataset.from_pandas(df)

# Set the maximum length to the full dataset
MAX_LENGTH = len(dataset)

# Removing the selection of only 500 samples
# If you want the full dataset, you can directly use 'dataset' without selecting a subset.

# Creating templates
prompt_template = """Below is an instruction that describes a task.
                     Write a response that appropriately completes the request.
                     Instruction: {instruction}\n Response:"""
answer_template = """{response}"""

# Function to add keys in the dictionary for prompt, answer, and whole text
def _add_text(rec):
    instruction = rec["question"]  # 사용자의 질문 -> 모델의 입력
    response = rec["answer"]      # 상담자의 답변 -> 모델의 출력

    # Check if both exist
    if not instruction:
        raise ValueError(f"Expected an instruction in: {rec}")
    if not response:
        raise ValueError(f"Expected a response in: {rec}")

    # Create prompt and text
    rec["prompt"] = prompt_template.format(instruction=instruction)
    rec["answer"] = answer_template.format(response=response)
    rec["text"] = f"{rec['prompt']}\n{rec['answer']}"
    return rec


# Running through all samples (no selection, using full dataset)
print("Before:", dataset[12])
dataset = dataset.map(_add_text)

print("After:", dataset[12])

"""## 2. foundation model 로딩
- GPT-2 :  주어진 텍스트의 다음 단어를 잘 예측할 수 있도록 학습된 언어모델이며 문장 생성에 최적화
- skt/kogpt2-base-v2

"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "skt/kogpt2-base-v2"

# 로드할 모델과 토크나이저 설정
tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')
tokenizer.pad_token = tokenizer.eos_token  # padding token을 eos token으로 설정

# causalLM
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # 적절한 device 선택
    torch_dtype=torch.float16
)

# 모델의 입력 토큰 임베딩 크기 조정 (필요한 경우)
model.resize_token_embeddings(len(tokenizer))

"""## 3. 배치 데이터 준비

- 각 데이터 최대 길이 256
- causal LM
- train data : 86% / test data : 14%
"""

from functools import partial
from datasets import Dataset

MAX_LENGTH = 256

# Function to generate token embeddings from text part of batch
def _preprocess_batch(batch: dict):
    model_inputs = tokenizer(batch["text"], max_length=MAX_LENGTH, truncation=True, padding='max_length')
    model_inputs["labels"] = model_inputs["input_ids"]  # No need for deepcopy
    return model_inputs

_preprocessing_function = partial(_preprocess_batch)

# 전체 데이터셋에 대해 전처리 함수 적용
encoded_dataset = dataset.map(
    _preprocessing_function,
    batched=True,
    remove_columns=["type", "environment", "prompt"]
)

# Tokenized input 길이가 MAX_LENGTH 이하인 데이터만 필터링
processed_dataset = encoded_dataset.filter(lambda rec: len(rec["input_ids"]) <= MAX_LENGTH)

# 데이터셋을 학습용과 검증용으로 나누기 (14%를 검증용으로, 나머지는 학습용)
split_dataset = processed_dataset.train_test_split(test_size=0.14, seed=0)
print(split_dataset)

"""## 4. LoRA configuration
- PEFT 어뎁터
- task type = CAUSAL_LM
- target modules = "c_proj"
"""

!pip install --upgrade peft

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

LORA_R = 32 #516
LORA_ALPHA = 128 # 1024
LORA_DROPOUT = 0.1
# Define LoRA Config

lora_config = LoraConfig(
                 r = LORA_R, # 저차원 근사의 차원의 크기. 크면 모델 성능에 영향 큼
                 lora_alpha = LORA_ALPHA, # 업데이트 가중치. 스케일링을 조정하는 파라미터. 크면 LoRA 파라미터 중요, 작으면 LoRA 영향 감소
                 lora_dropout = LORA_DROPOUT, # 과적합 방지. 0.1~0.2 사이 값 이용
                 bias="none",
                 task_type="CAUSAL_LM",  # causal language model 일반적
                 target_modules=["c_proj"],  # RoBERTa의 attention 모듈
                 #target_modules=["query_key_value"],
)


# Prepare int-8 model for training - utility function that prepares a PyTorch model for int8 quantization training. <https://huggingface.co/docs/peft/task_guides/int8-asr>
model = prepare_model_for_kbit_training(model)   # 일부 파라미터 int-8 양자화 하여 메모리 사용량 줄이고, 큰 모델 효율적 학습
# initialize the model with the LoRA framework
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# T5 LLM 모델의 파라미터 레이어 확인
from transformers import AutoModelForCausalLM

# 모델 로드
model_id = "skt/kogpt2-base-v2"  # T5 모델 ID
model = AutoModelForCausalLM.from_pretrained(model_id)

# 모델의 파라미터 이름 확인
for name, param in model.named_parameters():
    print(name)

"""## 5. LoRA Training"""

torch.cuda.empty_cache()  # 캐시 메모리 해제

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

# 이미 split_dataset으로 훈련과 검증 데이터셋이 나누어져 있음
train_dataset = split_dataset['train']
test_dataset = split_dataset['test']

# 훈련 파라미터 설정
EPOCHS = 3
LEARNING_RATE = 1e-4
google_drive_path = "/content/drive/MyDrive/Deep Learning"
training_args = TrainingArguments(
                    output_dir=google_drive_path,
                    overwrite_output_dir=True,
                    fp16=True,
                    per_device_train_batch_size=8,
                    per_device_eval_batch_size=8,
                    learning_rate=LEARNING_RATE,
                    gradient_accumulation_steps=4,
                    save_steps=500,
                    num_train_epochs=EPOCHS,
                    logging_strategy="epoch",
                    save_strategy="epoch",
                    report_to="none",
)

# Data collator for gpt-2 model
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # Causal LM 작업에 맞게 Masked LM 비활성화
    pad_to_multiple_of=8,  # 패딩을 8의 배수로 설정
)

# Trainer 설정
trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,  # eval_dataset을 설정
        data_collator=data_collator,
)

model.config.use_cache = False  # inference 시 캐시 사용을 비활성화하여 경고 메시지 방지

# 모델 훈련
trainer.train()

# 훈련이 끝난 후 모델 저장
trainer.model.save_pretrained(google_drive_path)
trainer.save_model(google_drive_path)
trainer.model.config.save_pretrained(google_drive_path)


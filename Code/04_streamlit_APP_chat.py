import streamlit as st
import re
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate

CHROMA_PATH = "./chroma"  # ì—…ë¡œë“œëœ Chroma ë²¡í„° DB íŒŒì¼ ê²½ë¡œ
# ëª¨ë¸ ë° ë²¡í„° DB ê²½ë¡œ ì„¤ì •
base_model_path = r"C:\Users\bhks0\Desktop\LLM ì‘ìš©1\models\Foundation_model"  # LLM ëª¨ë¸ ê²½ë¡œ
peft_adapter_path = r"C:\Users\bhks0\Desktop\LLM ì‘ìš©1\models\Tuning_model"  # PEFT ì–´ëŒ‘í„° ê²½ë¡œ

# LLM ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(base_model_path)
base_model.resize_token_embeddings(len(tokenizer))  # tokenizerì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–´íœ˜ í¬ê¸°
peft_config = PeftConfig.from_pretrained(peft_adapter_path, local_files_only=True)
model = PeftModel.from_pretrained(base_model, peft_adapter_path, local_files_only=True)

# GPU ì‚¬ìš© ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

modelPath = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2" # open source embedding model 

# Create a dictionary with model configuration options, specifying to use the CPU for computations
model_kwargs = {'device': 'cpu'}

# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False
encode_kwargs = {'normalize_embeddings': True}

# Initialize an instance of HuggingFaceEmbeddings with the specified parameters
embeddings = HuggingFaceEmbeddings(
    model_name=modelPath,     # Provide the pre-trained model's path
    model_kwargs=model_kwargs, # Pass the model configuration options
    encode_kwargs=encode_kwargs # Pass the encoding options
)

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="Chatbot with RAG", page_icon="ğŸ¤–")
st.title("ë™í–‰")
st.write("**ğŸŒ³ê³„ì† í•¨ê»˜ ìˆì„ê²Œìš”ğŸŒ³**")

# ì¤‘ë³µ ì œê±°
def clean_response(text):
    """Remove duplicate words, sentences, and excessive repetition."""
    sentences = text.split(". ")  # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    unique_sentences = list(dict.fromkeys(sentences))  # ì¤‘ë³µ ì œê±°
    cleaned_text = ". ".join(unique_sentences).strip()

    # íŠ¹ì • ë‹¨ì–´ë‚˜ ë¬¸ì¥ì˜ ë°˜ë³µ ì œê±°
    excessive_patterns = re.findall(r"(\b\w+\b)( \1)+", cleaned_text)  # ë°˜ë³µ ë‹¨ì–´ ê°ì§€
    for pattern in excessive_patterns:
        cleaned_text = re.sub(rf"{pattern[0]}( {pattern[0]})+", pattern[0], cleaned_text)

    return cleaned_text

# RAG ê²€ìƒ‰ ë° ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜
def query_rag(query_text):
    # ë²¡í„° DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë¬¸ì„œ ë°˜í™˜
    embedding_function = embeddings

    # Load the existing Chroma database
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
    
    results = db.similarity_search_with_relevance_scores(query_text, k=2)
    
    # ìœ ì‚¬ë„ ì ìˆ˜ í•„í„°ë§
    valid_results = [res for res in results if -2 <= res[1] <= 2]
    if not valid_results:
        return "ê´€ë ¨ëœ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ ê²°ê³¼ ì„ íƒ
    best_result = max(valid_results, key=lambda x: x[1])
    return best_result[0].page_content


def model_run(context_text, query_text, prompt_template):
    # ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    context_text = " ".join([context_text])

    # Prompt í…œí”Œë¦¿ ìƒì„±
    prompt_template = PromptTemplate.from_template(prompt_template)
    prompt = prompt_template.format(context=context_text, question=query_text)

    # pad_token_idë¥¼ eos_token_idë¡œ ì„¤ì •
    if tokenizer.pad_token is None:
      tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ì— ì…ë ¥í•˜ê³  ì‘ë‹µ ìƒì„±
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(device)

    # ëª¨ë¸ ì¶”ë¡  (ìƒì„± ê²°ê³¼)
    with torch.no_grad():
      outputs = model.generate(
        input_ids=inputs["input_ids"], 
        attention_mask=inputs['attention_mask'], 
        pad_token_id=tokenizer.pad_token_id,
        max_length=256, num_return_sequences=1,
        no_repeat_ngram_size=3,  # 3ê·¸ë¨ ë°˜ë³µ ë°©ì§€
        repetition_penalty=2.0,   # ë°˜ë³µ ìƒì„± ì–µì œ
        temperature=0.7  #ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„± ì„¤ì •
        )

    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return clean_response(response_text)


def generate_response(input_text):
  query_text = input_text
  prompt_template = """
    Context: {context} \n
    Question: {question} \n
  """
  # Let's call our function we have defined
  rag_text = query_rag(query_text)
  response_text = model_run(rag_text, query_text, prompt_template)
  
  # and finally, inspect our final response!
  st.info(response_text)

with st.form("my_form"):
    text = st.text_area(
        "Enter text:",
        " ",
    )
    submitted = st.form_submit_button("Submit")
    if submitted :
        generate_response(text)

